# LMMs-based-IQA

- **[ICLR 2024]** [Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision](https://arxiv.org/abs/2309.14181), Wu et al. [Github](https://github.com/Q-Future/Q-Bench) | [Bibtex](https://arxiv.org/bibtex/2309.14181) | [Dataset](https://github.com/Q-Future/Q-Bench/releases/tag/v1.0.1.1014datarelease)

- **[AAAI 2023]** [Exploring CLIP for Assessing the Look and Feel of Images](https://arxiv.org/abs/2207.12396), Wang et al. [Github](https://github.com/IceClear/CLIP-IQA) | [Bibtex](https://arxiv.org/bibtex/2207.12396)

- **[arXiv 2023]** [Q-Boost: On Visual Quality Assessment Ability of Low-level Multi-Modality Foundation Models](https://arxiv.org/abs/2312.15300), Zhang et al. [Bibtex](https://arxiv.org/bibtex/2312.15300)

- **[arXiv 2023]** [Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-defined Levels](https://arxiv.org/abs/2312.17090), Wu et al. [Project](https://q-align.github.io/) | [Github](https://github.com/Q-Future/Q-Align) | [Bibtex](https://arxiv.org/bibtex/2312.17090)

- **[CVPR 2023]** [Blind Image Quality Assessment via Vision-Language Correspondence: A Multitask Learning Perspective](https://arxiv.org/abs/2303.14968), Zhang et al. [Github](https://github.com/zwx8981/LIQE) | [Bibtex](https://arxiv.org/bibtex/2303.14968)

- **[arXiv 2024]** [CLIP-Guided Attribute Aware Pretraining for Generalizable Image Quality Assessment](https://arxiv.org/abs/2406.01020), Kwon et al. [Bibtex](https://arxiv.org/bibtex/2406.01020)

- **[arXiv 2024]** [Q-Ground: Image Quality Grounding with Large Multi-modality Models](https://arxiv.org/abs/2407.17035), Chen et al. [Github](https://github.com/Q-Future/Q-Ground) | [Bibtex](https://arxiv.org/bibtex/2407.17035)

- **[arXiv 2023]** [Depicting Beyond Scores: Advancing Image Quality Assessment through Multi-Modal Language Models](https://arxiv.org/abs/2312.08962), You et al. [Project](https://depictqa.github.io/) | [Bibtex](https://arxiv.org/bibtex/2312.08962)

- **[arXiv 2023]** [Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models](https://arxiv.org/abs/2311.06783), Wu et al. [Project](https://q-future.github.io/Q-Instruct/) | [Bibtex](https://arxiv.org/bibtex/2311.06783)

- **[arXiv 2024]** [Descriptive Image Quality Assessment in the Wild](https://arxiv.org/abs/2405.18842), You et al. [Project](https://depictqa.github.io/depictqa-wild/) | [Github](https://github.com/XPixelGroup/DepictQA) | [Bibtex](https://arxiv.org/bibtex/2405.18842)

- **[arXiv 2024]** [Towards Open-ended Visual Quality Comparison](https://arxiv.org/abs/2402.16641), Wu et al. [Project](https://huggingface.co/q-future/co-instruct) | [Bibtex](https://arxiv.org/bibtex/2402.16641)

- **[arXiv 2024]** [Adaptive Image Quality Assessment via Teaching Large Multimodal Model to Compare](https://arxiv.org/abs/2405.19298), Zhu et al. [Project](https://compare2score.github.io/) | [Github](https://github.com/Q-Future/Compare2Score) | [Bibtex](https://arxiv.org/bibtex/2405.19298)
